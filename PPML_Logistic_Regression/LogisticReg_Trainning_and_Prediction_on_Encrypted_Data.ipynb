{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542abdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tenseal as ts\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ff3f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(x, y, ratio=0.3):\n",
    "    idxs = list(range(len(x)))\n",
    "    random.shuffle(idxs)\n",
    "    split_idx = int(len(x)*ratio)\n",
    "    test_idxs, train_idxs = idxs[:split_idx], idxs[split_idx:]\n",
    "    return x[train_idxs], y[train_idxs], x[test_idxs], y[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fbd1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_heart_disease_data():\n",
    "    data = pd.read_csv(\"../data/framingham.csv\")\n",
    "    # Drop target columns\n",
    "    X = data.drop(['TenYearCHD'], axis=1, inplace=False)\n",
    "    Y = data['TenYearCHD']\n",
    "    X = X.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    # Standardize data\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    X = torch.tensor(X.values).float()\n",
    "    Y = torch.tensor(Y.values).float().unsqueeze(1)\n",
    "    \n",
    "    return split_train_test(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ea25f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Data summary #############\n",
      "x_train has shape: torch.Size([2967, 15])\n",
      "y_train has shape: torch.Size([2967, 1])\n",
      "x_test has shape: torch.Size([1271, 15])\n",
      "y_test has shape: torch.Size([1271, 1])\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = prepare_heart_disease_data()\n",
    "\n",
    "print(\"############# Data summary #############\")\n",
    "print(f\"x_train has shape: {x_train.shape}\")\n",
    "print(f\"y_train has shape: {y_train.shape}\")\n",
    "print(f\"x_test has shape: {x_test.shape}\")\n",
    "print(f\"y_test has shape: {y_test.shape}\")\n",
    "print(\"#######################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18ad78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(LR, self).__init__()\n",
    "        self.lr = torch.nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.lr(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7aba6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = x_train.shape[1]\n",
    "model = LR(n_features)\n",
    "# use gradient descent with a learning_rate=1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# use Binary Cross Entropy Loss\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b112110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.5273855328559875\n",
      "Loss at epoch 2: 0.5268250107765198\n",
      "Loss at epoch 3: 0.5262671113014221\n",
      "Loss at epoch 4: 0.5257118940353394\n",
      "Loss at epoch 5: 0.5251592993736267\n",
      "Loss at epoch 6: 0.5246093273162842\n",
      "Loss at epoch 7: 0.5240618586540222\n",
      "Loss at epoch 8: 0.5235170125961304\n",
      "Loss at epoch 9: 0.5229747891426086\n",
      "Loss at epoch 10: 0.5224349498748779\n",
      "Loss at epoch 11: 0.5218977928161621\n",
      "Loss at epoch 12: 0.5213630795478821\n",
      "Loss at epoch 13: 0.5208309292793274\n",
      "Loss at epoch 14: 0.5203012824058533\n",
      "Loss at epoch 15: 0.5197740793228149\n",
      "Loss at epoch 16: 0.5192493200302124\n",
      "Loss at epoch 17: 0.5187271237373352\n",
      "Loss at epoch 18: 0.5182072520256042\n",
      "Loss at epoch 19: 0.5176898837089539\n",
      "Loss at epoch 20: 0.5171748995780945\n",
      "Loss at epoch 21: 0.5166622400283813\n",
      "Loss at epoch 22: 0.5161521434783936\n",
      "Loss at epoch 23: 0.5156442523002625\n",
      "Loss at epoch 24: 0.5151388049125671\n",
      "Loss at epoch 25: 0.5146357417106628\n",
      "Loss at epoch 26: 0.51413494348526\n",
      "Loss at epoch 27: 0.5136364698410034\n",
      "Loss at epoch 28: 0.5131403803825378\n",
      "Loss at epoch 29: 0.512646496295929\n",
      "Loss at epoch 30: 0.5121549963951111\n",
      "Loss at epoch 31: 0.5116657614707947\n",
      "Loss at epoch 32: 0.511178731918335\n",
      "Loss at epoch 33: 0.5106940269470215\n",
      "Loss at epoch 34: 0.5102114677429199\n",
      "Loss at epoch 35: 0.5097311735153198\n",
      "Loss at epoch 36: 0.5092531442642212\n",
      "Loss at epoch 37: 0.5087772607803345\n",
      "Loss at epoch 38: 0.5083036422729492\n",
      "Loss at epoch 39: 0.5078321099281311\n",
      "Loss at epoch 40: 0.5073627829551697\n",
      "Loss at epoch 41: 0.5068957209587097\n",
      "Loss at epoch 42: 0.5064307451248169\n",
      "Loss at epoch 43: 0.5059677958488464\n",
      "Loss at epoch 44: 0.5055070519447327\n",
      "Loss at epoch 45: 0.505048394203186\n",
      "Loss at epoch 46: 0.5045918226242065\n",
      "Loss at epoch 47: 0.5041374564170837\n",
      "Loss at epoch 48: 0.5036851763725281\n",
      "Loss at epoch 49: 0.5032348036766052\n",
      "Loss at epoch 50: 0.5027865767478943\n",
      "Loss at epoch 51: 0.5023403763771057\n",
      "Loss at epoch 52: 0.5018962025642395\n",
      "Loss at epoch 53: 0.5014540553092957\n",
      "Loss at epoch 54: 0.5010139346122742\n",
      "Loss at epoch 55: 0.500575840473175\n",
      "Loss at epoch 56: 0.5001396536827087\n",
      "Loss at epoch 57: 0.49970555305480957\n",
      "Loss at epoch 58: 0.4992733299732208\n",
      "Loss at epoch 59: 0.49884316325187683\n",
      "Loss at epoch 60: 0.49841490387916565\n",
      "Loss at epoch 61: 0.4979885220527649\n",
      "Loss at epoch 62: 0.4975641965866089\n",
      "Loss at epoch 63: 0.49714168906211853\n",
      "Loss at epoch 64: 0.49672117829322815\n",
      "Loss at epoch 65: 0.4963025152683258\n",
      "Loss at epoch 66: 0.4958856999874115\n",
      "Loss at epoch 67: 0.4954708218574524\n",
      "Loss at epoch 68: 0.4950577914714813\n",
      "Loss at epoch 69: 0.4946466088294983\n",
      "Loss at epoch 70: 0.4942372143268585\n",
      "Loss at epoch 71: 0.49382975697517395\n",
      "Loss at epoch 72: 0.49342411756515503\n",
      "Loss at epoch 73: 0.49302029609680176\n",
      "Loss at epoch 74: 0.49261829257011414\n",
      "Loss at epoch 75: 0.4922180771827698\n",
      "Loss at epoch 76: 0.4918196499347687\n",
      "Loss at epoch 77: 0.49142298102378845\n",
      "Loss at epoch 78: 0.4910281300544739\n",
      "Loss at epoch 79: 0.4906350076198578\n",
      "Loss at epoch 80: 0.49024367332458496\n",
      "Loss at epoch 81: 0.48985403776168823\n",
      "Loss at epoch 82: 0.48946619033813477\n",
      "Loss at epoch 83: 0.4890799820423126\n",
      "Loss at epoch 84: 0.48869559168815613\n",
      "Loss at epoch 85: 0.4883129298686981\n",
      "Loss at epoch 86: 0.48793187737464905\n",
      "Loss at epoch 87: 0.4875524938106537\n",
      "Loss at epoch 88: 0.4871748387813568\n",
      "Loss at epoch 89: 0.48679885268211365\n",
      "Loss at epoch 90: 0.4864245355129242\n",
      "Loss at epoch 91: 0.48605185747146606\n",
      "Loss at epoch 92: 0.48568087816238403\n",
      "Loss at epoch 93: 0.48531147837638855\n",
      "Loss at epoch 94: 0.4849437475204468\n",
      "Loss at epoch 95: 0.48457765579223633\n",
      "Loss at epoch 96: 0.48421311378479004\n",
      "Loss at epoch 97: 0.4838502109050751\n",
      "Loss at epoch 98: 0.48348891735076904\n",
      "Loss at epoch 99: 0.4831291735172272\n",
      "Loss at epoch 100: 0.48277100920677185\n",
      "Loss at epoch 101: 0.4824144244194031\n",
      "Loss at epoch 102: 0.48205944895744324\n",
      "Loss at epoch 103: 0.4817059636116028\n",
      "Loss at epoch 104: 0.4813540577888489\n",
      "Loss at epoch 105: 0.48100370168685913\n",
      "Loss at epoch 106: 0.48065483570098877\n",
      "Loss at epoch 107: 0.4803074598312378\n",
      "Loss at epoch 108: 0.47996169328689575\n",
      "Loss at epoch 109: 0.4796173870563507\n",
      "Loss at epoch 110: 0.4792746305465698\n",
      "Loss at epoch 111: 0.47893327474594116\n",
      "Loss at epoch 112: 0.4785934388637543\n",
      "Loss at epoch 113: 0.47825512290000916\n",
      "Loss at epoch 114: 0.47791823744773865\n",
      "Loss at epoch 115: 0.47758278250694275\n",
      "Loss at epoch 116: 0.4772488474845886\n",
      "Loss at epoch 117: 0.47691628336906433\n",
      "Loss at epoch 118: 0.4765852093696594\n",
      "Loss at epoch 119: 0.47625553607940674\n",
      "Loss at epoch 120: 0.47592735290527344\n",
      "Loss at epoch 121: 0.4756004512310028\n",
      "Loss at epoch 122: 0.47527509927749634\n",
      "Loss at epoch 123: 0.47495102882385254\n",
      "Loss at epoch 124: 0.4746284782886505\n",
      "Loss at epoch 125: 0.47430717945098877\n",
      "Loss at epoch 126: 0.4739873707294464\n",
      "Loss at epoch 127: 0.4736688435077667\n",
      "Loss at epoch 128: 0.47335177659988403\n",
      "Loss at epoch 129: 0.4730360507965088\n",
      "Loss at epoch 130: 0.472721666097641\n",
      "Loss at epoch 131: 0.4724085330963135\n",
      "Loss at epoch 132: 0.4720968008041382\n",
      "Loss at epoch 133: 0.47178640961647034\n",
      "Loss at epoch 134: 0.4714773893356323\n",
      "Loss at epoch 135: 0.4711695909500122\n",
      "Loss at epoch 136: 0.4708631634712219\n",
      "Loss at epoch 137: 0.4705580472946167\n",
      "Loss at epoch 138: 0.47025418281555176\n",
      "Loss at epoch 139: 0.4699515998363495\n",
      "Loss at epoch 140: 0.46965035796165466\n",
      "Loss at epoch 141: 0.4693503677845001\n",
      "Loss at epoch 142: 0.46905165910720825\n",
      "Loss at epoch 143: 0.4687541425228119\n",
      "Loss at epoch 144: 0.468457967042923\n",
      "Loss at epoch 145: 0.46816307306289673\n",
      "Loss at epoch 146: 0.467869371175766\n",
      "Loss at epoch 147: 0.46757686138153076\n",
      "Loss at epoch 148: 0.4672856628894806\n",
      "Loss at epoch 149: 0.4669956862926483\n",
      "Loss at epoch 150: 0.4667068123817444\n",
      "Loss at epoch 151: 0.4664192795753479\n",
      "Loss at epoch 152: 0.46613290905952454\n",
      "Loss at epoch 153: 0.46584779024124146\n",
      "Loss at epoch 154: 0.4655637741088867\n",
      "Loss at epoch 155: 0.46528100967407227\n",
      "Loss at epoch 156: 0.4649994373321533\n",
      "Loss at epoch 157: 0.4647189974784851\n",
      "Loss at epoch 158: 0.46443971991539\n",
      "Loss at epoch 159: 0.4641616940498352\n",
      "Loss at epoch 160: 0.46388471126556396\n",
      "Loss at epoch 161: 0.4636090099811554\n",
      "Loss at epoch 162: 0.4633343517780304\n",
      "Loss at epoch 163: 0.46306082606315613\n",
      "Loss at epoch 164: 0.462788462638855\n",
      "Loss at epoch 165: 0.46251726150512695\n",
      "Loss at epoch 166: 0.46224719285964966\n",
      "Loss at epoch 167: 0.4619782567024231\n",
      "Loss at epoch 168: 0.4617103934288025\n",
      "Loss at epoch 169: 0.46144363284111023\n",
      "Loss at epoch 170: 0.4611779749393463\n",
      "Loss at epoch 171: 0.4609134793281555\n",
      "Loss at epoch 172: 0.4606499671936035\n",
      "Loss at epoch 173: 0.46038761734962463\n",
      "Loss at epoch 174: 0.4601263403892517\n",
      "Loss at epoch 175: 0.45986613631248474\n",
      "Loss at epoch 176: 0.45960697531700134\n",
      "Loss at epoch 177: 0.4593488872051239\n",
      "Loss at epoch 178: 0.45909184217453003\n",
      "Loss at epoch 179: 0.4588358998298645\n",
      "Loss at epoch 180: 0.45858094096183777\n",
      "Loss at epoch 181: 0.45832711458206177\n",
      "Loss at epoch 182: 0.45807427167892456\n",
      "Loss at epoch 183: 0.4578225016593933\n",
      "Loss at epoch 184: 0.45757168531417847\n",
      "Loss at epoch 185: 0.45732197165489197\n",
      "Loss at epoch 186: 0.4570732116699219\n",
      "Loss at epoch 187: 0.45682549476623535\n",
      "Loss at epoch 188: 0.4565787613391876\n",
      "Loss at epoch 189: 0.4563330113887787\n",
      "Loss at epoch 190: 0.4560883343219757\n",
      "Loss at epoch 191: 0.45584458112716675\n",
      "Loss at epoch 192: 0.45560187101364136\n",
      "Loss at epoch 193: 0.4553601145744324\n",
      "Loss at epoch 194: 0.45511940121650696\n",
      "Loss at epoch 195: 0.4548795819282532\n",
      "Loss at epoch 196: 0.454640656709671\n",
      "Loss at epoch 197: 0.4544028043746948\n",
      "Loss at epoch 198: 0.4541659653186798\n",
      "Loss at epoch 199: 0.45392996072769165\n",
      "Loss at epoch 200: 0.45369499921798706\n",
      "Loss at epoch 201: 0.4534609019756317\n",
      "Loss at epoch 202: 0.45322778820991516\n",
      "Loss at epoch 203: 0.4529955983161926\n",
      "Loss at epoch 204: 0.4527643322944641\n",
      "Loss at epoch 205: 0.452534019947052\n",
      "Loss at epoch 206: 0.4523046016693115\n",
      "Loss at epoch 207: 0.45207610726356506\n",
      "Loss at epoch 208: 0.4518485367298126\n",
      "Loss at epoch 209: 0.4516218900680542\n",
      "Loss at epoch 210: 0.451396107673645\n",
      "Loss at epoch 211: 0.45117124915122986\n",
      "Loss at epoch 212: 0.4509473145008087\n",
      "Loss at epoch 213: 0.45072421431541443\n",
      "Loss at epoch 214: 0.45050206780433655\n",
      "Loss at epoch 215: 0.4502807557582855\n",
      "Loss at epoch 216: 0.4500603675842285\n",
      "Loss at epoch 217: 0.4498407542705536\n",
      "Loss at epoch 218: 0.44962212443351746\n",
      "Loss at epoch 219: 0.4494042694568634\n",
      "Loss at epoch 220: 0.449187308549881\n",
      "Loss at epoch 221: 0.4489711821079254\n",
      "Loss at epoch 222: 0.44875597953796387\n",
      "Loss at epoch 223: 0.4485415816307068\n",
      "Loss at epoch 224: 0.4483279585838318\n",
      "Loss at epoch 225: 0.4481152594089508\n",
      "Loss at epoch 226: 0.44790342450141907\n",
      "Loss at epoch 227: 0.4476923644542694\n",
      "Loss at epoch 228: 0.447482168674469\n",
      "Loss at epoch 229: 0.44727277755737305\n",
      "Loss at epoch 230: 0.44706419110298157\n",
      "Loss at epoch 231: 0.44685640931129456\n",
      "Loss at epoch 232: 0.4466494619846344\n",
      "Loss at epoch 233: 0.4464433789253235\n",
      "Loss at epoch 234: 0.44623810052871704\n",
      "Loss at epoch 235: 0.4460335373878479\n",
      "Loss at epoch 236: 0.4458297789096832\n",
      "Loss at epoch 237: 0.4456269145011902\n",
      "Loss at epoch 238: 0.44542473554611206\n",
      "Loss at epoch 239: 0.4452233910560608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 240: 0.4450227618217468\n",
      "Loss at epoch 241: 0.4448230266571045\n",
      "Loss at epoch 242: 0.4446239769458771\n",
      "Loss at epoch 243: 0.44442570209503174\n",
      "Loss at epoch 244: 0.44422823190689087\n",
      "Loss at epoch 245: 0.4440315365791321\n",
      "Loss at epoch 246: 0.4438355565071106\n",
      "Loss at epoch 247: 0.4436403512954712\n",
      "Loss at epoch 248: 0.44344595074653625\n",
      "Loss at epoch 249: 0.44325220584869385\n",
      "Loss at epoch 250: 0.4430592656135559\n",
      "Loss at epoch 251: 0.4428670108318329\n",
      "Loss at epoch 252: 0.44267553091049194\n",
      "Loss at epoch 253: 0.44248485565185547\n",
      "Loss at epoch 254: 0.4422948360443115\n",
      "Loss at epoch 255: 0.44210559129714966\n",
      "Loss at epoch 256: 0.4419170022010803\n",
      "Loss at epoch 257: 0.44172918796539307\n",
      "Loss at epoch 258: 0.4415420591831207\n",
      "Loss at epoch 259: 0.4413556456565857\n",
      "Loss at epoch 260: 0.44117000699043274\n",
      "Loss at epoch 261: 0.4409849941730499\n",
      "Loss at epoch 262: 0.4408007264137268\n",
      "Loss at epoch 263: 0.440617173910141\n",
      "Loss at epoch 264: 0.4404342472553253\n",
      "Loss at epoch 265: 0.44025206565856934\n",
      "Loss at epoch 266: 0.44007059931755066\n",
      "Loss at epoch 267: 0.4398898184299469\n",
      "Loss at epoch 268: 0.43970969319343567\n",
      "Loss at epoch 269: 0.43953031301498413\n",
      "Loss at epoch 270: 0.43935149908065796\n",
      "Loss at epoch 271: 0.43917348980903625\n",
      "Loss at epoch 272: 0.43899601697921753\n",
      "Loss at epoch 273: 0.43881934881210327\n",
      "Loss at epoch 274: 0.43864327669143677\n",
      "Loss at epoch 275: 0.4384678900241852\n",
      "Loss at epoch 276: 0.43829306960105896\n",
      "Loss at epoch 277: 0.4381190240383148\n",
      "Loss at epoch 278: 0.4379456043243408\n",
      "Loss at epoch 279: 0.43777281045913696\n",
      "Loss at epoch 280: 0.4376007318496704\n",
      "Loss at epoch 281: 0.43742918968200684\n",
      "Loss at epoch 282: 0.43725836277008057\n",
      "Loss at epoch 283: 0.43708813190460205\n",
      "Loss at epoch 284: 0.43691858649253845\n",
      "Loss at epoch 285: 0.4367496371269226\n",
      "Loss at epoch 286: 0.4365813136100769\n",
      "Loss at epoch 287: 0.43641361594200134\n",
      "Loss at epoch 288: 0.4362465441226959\n",
      "Loss at epoch 289: 0.4360801577568054\n",
      "Loss at epoch 290: 0.4359143078327179\n",
      "Loss at epoch 291: 0.4357491433620453\n",
      "Loss at epoch 292: 0.43558451533317566\n",
      "Loss at epoch 293: 0.43542057275772095\n",
      "Loss at epoch 294: 0.4352571964263916\n",
      "Loss at epoch 295: 0.4350944757461548\n",
      "Loss at epoch 296: 0.43493223190307617\n",
      "Loss at epoch 297: 0.43477076292037964\n",
      "Loss at epoch 298: 0.4346097409725189\n",
      "Loss at epoch 299: 0.4344494044780731\n",
      "Loss at epoch 300: 0.4342896044254303\n",
      "Loss at epoch 301: 0.43413046002388\n",
      "Loss at epoch 302: 0.4339718222618103\n",
      "Loss at epoch 303: 0.43381378054618835\n",
      "Loss at epoch 304: 0.43365636467933655\n",
      "Loss at epoch 305: 0.43349945545196533\n",
      "Loss at epoch 306: 0.43334320187568665\n",
      "Loss at epoch 307: 0.4331875145435333\n",
      "Loss at epoch 308: 0.433032363653183\n",
      "Loss at epoch 309: 0.432877779006958\n",
      "Loss at epoch 310: 0.4327237606048584\n",
      "Loss at epoch 311: 0.43257033824920654\n",
      "Loss at epoch 312: 0.43241745233535767\n",
      "Loss at epoch 313: 0.432265043258667\n",
      "Loss at epoch 314: 0.43211331963539124\n",
      "Loss at epoch 315: 0.43196210265159607\n",
      "Loss at epoch 316: 0.4318113625049591\n",
      "Loss at epoch 317: 0.4316612780094147\n",
      "Loss at epoch 318: 0.43151167035102844\n",
      "Loss at epoch 319: 0.43136265873908997\n",
      "Loss at epoch 320: 0.4312141537666321\n",
      "Loss at epoch 321: 0.4310661852359772\n",
      "Loss at epoch 322: 0.43091878294944763\n",
      "Loss at epoch 323: 0.4307718575000763\n",
      "Loss at epoch 324: 0.43062546849250793\n",
      "Loss at epoch 325: 0.43047967553138733\n",
      "Loss at epoch 326: 0.4303343594074249\n",
      "Loss at epoch 327: 0.4301896095275879\n",
      "Loss at epoch 328: 0.43004536628723145\n",
      "Loss at epoch 329: 0.42990151047706604\n",
      "Loss at epoch 330: 0.42975834012031555\n",
      "Loss at epoch 331: 0.4296156167984009\n",
      "Loss at epoch 332: 0.4294734001159668\n",
      "Loss at epoch 333: 0.4293316602706909\n",
      "Loss at epoch 334: 0.4291905462741852\n",
      "Loss at epoch 335: 0.4290498197078705\n",
      "Loss at epoch 336: 0.42890965938568115\n",
      "Loss at epoch 337: 0.42876997590065\n",
      "Loss at epoch 338: 0.4286307394504547\n",
      "Loss at epoch 339: 0.42849212884902954\n",
      "Loss at epoch 340: 0.4283539354801178\n",
      "Loss at epoch 341: 0.42821618914604187\n",
      "Loss at epoch 342: 0.4280789792537689\n",
      "Loss at epoch 343: 0.42794230580329895\n",
      "Loss at epoch 344: 0.4278060495853424\n",
      "Loss at epoch 345: 0.42767027020454407\n",
      "Loss at epoch 346: 0.4275350272655487\n",
      "Loss at epoch 347: 0.4274002015590668\n",
      "Loss at epoch 348: 0.4272659122943878\n",
      "Loss at epoch 349: 0.42713209986686707\n",
      "Loss at epoch 350: 0.42699873447418213\n",
      "Loss at epoch 351: 0.4268658459186554\n",
      "Loss at epoch 352: 0.4267333745956421\n",
      "Loss at epoch 353: 0.42660146951675415\n",
      "Loss at epoch 354: 0.42646995186805725\n",
      "Loss at epoch 355: 0.42633891105651855\n",
      "Loss at epoch 356: 0.4262083172798157\n",
      "Loss at epoch 357: 0.4260782301425934\n",
      "Loss at epoch 358: 0.42594853043556213\n",
      "Loss at epoch 359: 0.4258193373680115\n",
      "Loss at epoch 360: 0.425690621137619\n",
      "Loss at epoch 361: 0.42556232213974\n",
      "Loss at epoch 362: 0.425434410572052\n",
      "Loss at epoch 363: 0.425307035446167\n",
      "Loss at epoch 364: 0.4251800775527954\n",
      "Loss at epoch 365: 0.42505356669425964\n",
      "Loss at epoch 366: 0.4249275028705597\n",
      "Loss at epoch 367: 0.42480185627937317\n",
      "Loss at epoch 368: 0.42467665672302246\n",
      "Loss at epoch 369: 0.4245518743991852\n",
      "Loss at epoch 370: 0.4244275391101837\n",
      "Loss at epoch 371: 0.42430365085601807\n",
      "Loss at epoch 372: 0.42418015003204346\n",
      "Loss at epoch 373: 0.42405715584754944\n",
      "Loss at epoch 374: 0.4239344894886017\n",
      "Loss at epoch 375: 0.42381229996681213\n",
      "Loss at epoch 376: 0.4236905574798584\n",
      "Loss at epoch 377: 0.4235692024230957\n",
      "Loss at epoch 378: 0.4234482944011688\n",
      "Loss at epoch 379: 0.42332780361175537\n",
      "Loss at epoch 380: 0.4232076406478882\n",
      "Loss at epoch 381: 0.4230879545211792\n",
      "Loss at epoch 382: 0.4229687452316284\n",
      "Loss at epoch 383: 0.4228498339653015\n",
      "Loss at epoch 384: 0.4227313995361328\n",
      "Loss at epoch 385: 0.42261335253715515\n",
      "Loss at epoch 386: 0.42249569296836853\n",
      "Loss at epoch 387: 0.42237842082977295\n",
      "Loss at epoch 388: 0.42226165533065796\n",
      "Loss at epoch 389: 0.42214521765708923\n",
      "Loss at epoch 390: 0.42202916741371155\n",
      "Loss at epoch 391: 0.4219135642051697\n",
      "Loss at epoch 392: 0.42179831862449646\n",
      "Loss at epoch 393: 0.4216834008693695\n",
      "Loss at epoch 394: 0.42156893014907837\n",
      "Loss at epoch 395: 0.42145484685897827\n",
      "Loss at epoch 396: 0.4213411211967468\n",
      "Loss at epoch 397: 0.4212278127670288\n",
      "Loss at epoch 398: 0.421114981174469\n",
      "Loss at epoch 399: 0.4210023581981659\n",
      "Loss at epoch 400: 0.42089030146598816\n",
      "Loss at epoch 401: 0.42077845335006714\n",
      "Loss at epoch 402: 0.42066705226898193\n",
      "Loss at epoch 403: 0.42055603861808777\n",
      "Loss at epoch 404: 0.42044541239738464\n",
      "Loss at epoch 405: 0.42033514380455017\n",
      "Loss at epoch 406: 0.42022523283958435\n",
      "Loss at epoch 407: 0.42011573910713196\n",
      "Loss at epoch 408: 0.42000657320022583\n",
      "Loss at epoch 409: 0.41989776492118835\n",
      "Loss at epoch 410: 0.41978931427001953\n",
      "Loss at epoch 411: 0.4196813106536865\n",
      "Loss at epoch 412: 0.4195736050605774\n",
      "Loss at epoch 413: 0.4194662570953369\n",
      "Loss at epoch 414: 0.4193592369556427\n",
      "Loss at epoch 415: 0.4192526340484619\n",
      "Loss at epoch 416: 0.4191463887691498\n",
      "Loss at epoch 417: 0.4190404415130615\n",
      "Loss at epoch 418: 0.4189349412918091\n",
      "Loss at epoch 419: 0.4188297390937805\n",
      "Loss at epoch 420: 0.4187248647212982\n",
      "Loss at epoch 421: 0.41862040758132935\n",
      "Loss at epoch 422: 0.41851624846458435\n",
      "Loss at epoch 423: 0.418412446975708\n",
      "Loss at epoch 424: 0.41830897331237793\n",
      "Loss at epoch 425: 0.4182058572769165\n",
      "Loss at epoch 426: 0.41810303926467896\n",
      "Loss at epoch 427: 0.41800063848495483\n",
      "Loss at epoch 428: 0.417898565530777\n",
      "Loss at epoch 429: 0.4177968204021454\n",
      "Loss at epoch 430: 0.41769540309906006\n",
      "Loss at epoch 431: 0.417594313621521\n",
      "Loss at epoch 432: 0.4174935519695282\n",
      "Loss at epoch 433: 0.41739311814308167\n",
      "Loss at epoch 434: 0.4172930121421814\n",
      "Loss at epoch 435: 0.41719329357147217\n",
      "Loss at epoch 436: 0.41709381341934204\n",
      "Loss at epoch 437: 0.41699478030204773\n",
      "Loss at epoch 438: 0.4168959856033325\n",
      "Loss at epoch 439: 0.41679754853248596\n",
      "Loss at epoch 440: 0.4166994094848633\n",
      "Loss at epoch 441: 0.4166015684604645\n",
      "Loss at epoch 442: 0.4165041148662567\n",
      "Loss at epoch 443: 0.41640692949295044\n",
      "Loss at epoch 444: 0.41631007194519043\n",
      "Loss at epoch 445: 0.4162135124206543\n",
      "Loss at epoch 446: 0.4161173403263092\n",
      "Loss at epoch 447: 0.416021466255188\n",
      "Loss at epoch 448: 0.41592589020729065\n",
      "Loss at epoch 449: 0.41583052277565\n",
      "Loss at epoch 450: 0.4157356023788452\n",
      "Loss at epoch 451: 0.4156409800052643\n",
      "Loss at epoch 452: 0.41554659605026245\n",
      "Loss at epoch 453: 0.41545259952545166\n",
      "Loss at epoch 454: 0.4153588116168976\n",
      "Loss at epoch 455: 0.41526544094085693\n",
      "Loss at epoch 456: 0.415172278881073\n",
      "Loss at epoch 457: 0.41507941484451294\n",
      "Loss at epoch 458: 0.41498690843582153\n",
      "Loss at epoch 459: 0.414894700050354\n",
      "Loss at epoch 460: 0.4148027300834656\n",
      "Loss at epoch 461: 0.4147111475467682\n",
      "Loss at epoch 462: 0.4146197438240051\n",
      "Loss at epoch 463: 0.4145287275314331\n",
      "Loss at epoch 464: 0.4144379794597626\n",
      "Loss at epoch 465: 0.4143475890159607\n",
      "Loss at epoch 466: 0.41425734758377075\n",
      "Loss at epoch 467: 0.41416749358177185\n",
      "Loss at epoch 468: 0.41407787799835205\n",
      "Loss at epoch 469: 0.4139886200428009\n",
      "Loss at epoch 470: 0.41389957070350647\n",
      "Loss at epoch 471: 0.4138108193874359\n",
      "Loss at epoch 472: 0.413722425699234\n",
      "Loss at epoch 473: 0.4136342406272888\n",
      "Loss at epoch 474: 0.4135463535785675\n",
      "Loss at epoch 475: 0.41345876455307007\n",
      "Loss at epoch 476: 0.41337141394615173\n",
      "Loss at epoch 477: 0.41328445076942444\n",
      "Loss at epoch 478: 0.4131976366043091\n",
      "Loss at epoch 479: 0.41311115026474\n",
      "Loss at epoch 480: 0.4130249619483948\n",
      "Loss at epoch 481: 0.41293901205062866\n",
      "Loss at epoch 482: 0.41285333037376404\n",
      "Loss at epoch 483: 0.4127679169178009\n",
      "Loss at epoch 484: 0.41268283128738403\n",
      "Loss at epoch 485: 0.41259798407554626\n",
      "Loss at epoch 486: 0.4125134348869324\n",
      "Loss at epoch 487: 0.4124290943145752\n",
      "Loss at epoch 488: 0.4123449921607971\n",
      "Loss at epoch 489: 0.4122612178325653\n",
      "Loss at epoch 490: 0.4121777415275574\n",
      "Loss at epoch 491: 0.41209444403648376\n",
      "Loss at epoch 492: 0.4120115041732788\n",
      "Loss at epoch 493: 0.41192877292633057\n",
      "Loss at epoch 494: 0.4118463099002838\n",
      "Loss at epoch 495: 0.41176411509513855\n",
      "Loss at epoch 496: 0.4116821587085724\n",
      "Loss at epoch 497: 0.4116004407405853\n",
      "Loss at epoch 498: 0.41151902079582214\n",
      "Loss at epoch 499: 0.41143783926963806\n",
      "Loss at epoch 500: 0.41135692596435547\n",
      "Loss at epoch 501: 0.4112762212753296\n",
      "Loss at epoch 502: 0.41119584441185\n",
      "Loss at epoch 503: 0.4111156165599823\n",
      "Loss at epoch 504: 0.4110357463359833\n",
      "Loss at epoch 505: 0.4109560251235962\n",
      "Loss at epoch 506: 0.41087666153907776\n",
      "Loss at epoch 507: 0.41079744696617126\n",
      "Loss at epoch 508: 0.41071850061416626\n",
      "Loss at epoch 509: 0.41063985228538513\n",
      "Loss at epoch 510: 0.4105614125728607\n",
      "Loss at epoch 511: 0.4104832410812378\n",
      "Loss at epoch 512: 0.4104052484035492\n",
      "Loss at epoch 513: 0.41032761335372925\n",
      "Loss at epoch 514: 0.41025009751319885\n",
      "Loss at epoch 515: 0.4101729094982147\n",
      "Loss at epoch 516: 0.4100959002971649\n",
      "Loss at epoch 517: 0.4100191593170166\n",
      "Loss at epoch 518: 0.4099426865577698\n",
      "Loss at epoch 519: 0.4098663926124573\n",
      "Loss at epoch 520: 0.4097903370857239\n",
      "Loss at epoch 521: 0.40971454977989197\n",
      "Loss at epoch 522: 0.40963900089263916\n",
      "Loss at epoch 523: 0.4095636308193207\n",
      "Loss at epoch 524: 0.4094885587692261\n",
      "Loss at epoch 525: 0.4094136655330658\n",
      "Loss at epoch 526: 0.409339040517807\n",
      "Loss at epoch 527: 0.40926459431648254\n",
      "Loss at epoch 528: 0.40919041633605957\n",
      "Loss at epoch 529: 0.4091164469718933\n",
      "Loss at epoch 530: 0.4090427756309509\n",
      "Loss at epoch 531: 0.4089692533016205\n",
      "Loss at epoch 532: 0.40889596939086914\n",
      "Loss at epoch 533: 0.4088228940963745\n",
      "Loss at epoch 534: 0.4087500274181366\n",
      "Loss at epoch 535: 0.40867745876312256\n",
      "Loss at epoch 536: 0.40860509872436523\n",
      "Loss at epoch 537: 0.40853288769721985\n",
      "Loss at epoch 538: 0.40846097469329834\n",
      "Loss at epoch 539: 0.40838924050331116\n",
      "Loss at epoch 540: 0.4083177447319031\n",
      "Loss at epoch 541: 0.40824639797210693\n",
      "Loss at epoch 542: 0.40817540884017944\n",
      "Loss at epoch 543: 0.4081045091152191\n",
      "Loss at epoch 544: 0.4080338776111603\n",
      "Loss at epoch 545: 0.40796345472335815\n",
      "Loss at epoch 546: 0.4078931510448456\n",
      "Loss at epoch 547: 0.40782320499420166\n",
      "Loss at epoch 548: 0.40775343775749207\n",
      "Loss at epoch 549: 0.4076838195323944\n",
      "Loss at epoch 550: 0.40761446952819824\n",
      "Loss at epoch 551: 0.4075453281402588\n",
      "Loss at epoch 552: 0.4074763357639313\n",
      "Loss at epoch 553: 0.40740758180618286\n",
      "Loss at epoch 554: 0.40733903646469116\n",
      "Loss at epoch 555: 0.40727075934410095\n",
      "Loss at epoch 556: 0.4072026312351227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 557: 0.40713468194007874\n",
      "Loss at epoch 558: 0.4070669710636139\n",
      "Loss at epoch 559: 0.40699946880340576\n",
      "Loss at epoch 560: 0.40693214535713196\n",
      "Loss at epoch 561: 0.40686503052711487\n",
      "Loss at epoch 562: 0.4067980945110321\n",
      "Loss at epoch 563: 0.40673139691352844\n",
      "Loss at epoch 564: 0.4066648781299591\n",
      "Loss at epoch 565: 0.40659859776496887\n",
      "Loss at epoch 566: 0.40653252601623535\n",
      "Loss at epoch 567: 0.4064665734767914\n",
      "Loss at epoch 568: 0.4064008593559265\n",
      "Loss at epoch 569: 0.40633538365364075\n",
      "Loss at epoch 570: 0.40627002716064453\n",
      "Loss at epoch 571: 0.4062049090862274\n",
      "Loss at epoch 572: 0.40613994002342224\n",
      "Loss at epoch 573: 0.40607523918151855\n",
      "Loss at epoch 574: 0.40601062774658203\n",
      "Loss at epoch 575: 0.405946284532547\n",
      "Loss at epoch 576: 0.4058821499347687\n",
      "Loss at epoch 577: 0.40581807494163513\n",
      "Loss at epoch 578: 0.40575435757637024\n",
      "Loss at epoch 579: 0.4056906998157501\n",
      "Loss at epoch 580: 0.4056273400783539\n",
      "Loss at epoch 581: 0.4055640995502472\n",
      "Loss at epoch 582: 0.4055010676383972\n",
      "Loss at epoch 583: 0.40543821454048157\n",
      "Loss at epoch 584: 0.40537554025650024\n",
      "Loss at epoch 585: 0.40531301498413086\n",
      "Loss at epoch 586: 0.4052507281303406\n",
      "Loss at epoch 587: 0.405188649892807\n",
      "Loss at epoch 588: 0.4051267206668854\n",
      "Loss at epoch 589: 0.40506497025489807\n",
      "Loss at epoch 590: 0.4050034284591675\n",
      "Loss at epoch 591: 0.40494200587272644\n",
      "Loss at epoch 592: 0.4048808217048645\n",
      "Loss at epoch 593: 0.4048197865486145\n",
      "Loss at epoch 594: 0.40475890040397644\n",
      "Loss at epoch 595: 0.4046982526779175\n",
      "Loss at epoch 596: 0.40463775396347046\n",
      "Loss at epoch 597: 0.40457746386528015\n",
      "Loss at epoch 598: 0.4045173227787018\n",
      "Loss at epoch 599: 0.40445733070373535\n",
      "Loss at epoch 600: 0.40439754724502563\n",
      "Loss at epoch 601: 0.40433794260025024\n",
      "Loss at epoch 602: 0.4042784869670868\n",
      "Loss at epoch 603: 0.40421923995018005\n",
      "Loss at epoch 604: 0.40416011214256287\n",
      "Loss at epoch 605: 0.4041011929512024\n",
      "Loss at epoch 606: 0.40404245257377625\n",
      "Loss at epoch 607: 0.40398386120796204\n",
      "Loss at epoch 608: 0.40392547845840454\n",
      "Loss at epoch 609: 0.4038672149181366\n",
      "Loss at epoch 610: 0.40380915999412537\n",
      "Loss at epoch 611: 0.4037512540817261\n",
      "Loss at epoch 612: 0.4036935269832611\n",
      "Loss at epoch 613: 0.4036359488964081\n",
      "Loss at epoch 614: 0.40357857942581177\n",
      "Loss at epoch 615: 0.4035212993621826\n",
      "Loss at epoch 616: 0.4034642279148102\n",
      "Loss at epoch 617: 0.40340733528137207\n",
      "Loss at epoch 618: 0.4033505618572235\n",
      "Loss at epoch 619: 0.4032939374446869\n",
      "Loss at epoch 620: 0.40323758125305176\n",
      "Loss at epoch 621: 0.4031812846660614\n",
      "Loss at epoch 622: 0.40312522649765015\n",
      "Loss at epoch 623: 0.40306925773620605\n",
      "Loss at epoch 624: 0.4030134975910187\n",
      "Loss at epoch 625: 0.4029579162597656\n",
      "Loss at epoch 626: 0.40290242433547974\n",
      "Loss at epoch 627: 0.40284717082977295\n",
      "Loss at epoch 628: 0.4027920365333557\n",
      "Loss at epoch 629: 0.402737021446228\n",
      "Loss at epoch 630: 0.40268218517303467\n",
      "Loss at epoch 631: 0.40262749791145325\n",
      "Loss at epoch 632: 0.4025730490684509\n",
      "Loss at epoch 633: 0.40251868963241577\n",
      "Loss at epoch 634: 0.40246444940567017\n",
      "Loss at epoch 635: 0.40241044759750366\n",
      "Loss at epoch 636: 0.4023565351963043\n",
      "Loss at epoch 637: 0.4023027718067169\n",
      "Loss at epoch 638: 0.40224921703338623\n",
      "Loss at epoch 639: 0.4021957516670227\n",
      "Loss at epoch 640: 0.4021425247192383\n",
      "Loss at epoch 641: 0.40208935737609863\n",
      "Loss at epoch 642: 0.4020364284515381\n",
      "Loss at epoch 643: 0.4019835889339447\n",
      "Loss at epoch 644: 0.4019308388233185\n",
      "Loss at epoch 645: 0.40187838673591614\n",
      "Loss at epoch 646: 0.4018259346485138\n",
      "Loss at epoch 647: 0.40177375078201294\n",
      "Loss at epoch 648: 0.40172168612480164\n",
      "Loss at epoch 649: 0.4016697108745575\n",
      "Loss at epoch 650: 0.40161794424057007\n",
      "Loss at epoch 651: 0.4015662372112274\n",
      "Loss at epoch 652: 0.4015147387981415\n",
      "Loss at epoch 653: 0.40146344900131226\n",
      "Loss at epoch 654: 0.4014121890068054\n",
      "Loss at epoch 655: 0.4013611078262329\n",
      "Loss at epoch 656: 0.4013102054595947\n",
      "Loss at epoch 657: 0.4012594223022461\n",
      "Loss at epoch 658: 0.4012087881565094\n",
      "Loss at epoch 659: 0.40115830302238464\n",
      "Loss at epoch 660: 0.40110793709754944\n",
      "Loss at epoch 661: 0.40105774998664856\n",
      "Loss at epoch 662: 0.40100765228271484\n",
      "Loss at epoch 663: 0.40095773339271545\n",
      "Loss at epoch 664: 0.4009079039096832\n",
      "Loss at epoch 665: 0.4008583426475525\n",
      "Loss at epoch 666: 0.40080875158309937\n",
      "Loss at epoch 667: 0.40075939893722534\n",
      "Loss at epoch 668: 0.4007101356983185\n",
      "Loss at epoch 669: 0.4006611108779907\n",
      "Loss at epoch 670: 0.4006120562553406\n",
      "Loss at epoch 671: 0.40056321024894714\n",
      "Loss at epoch 672: 0.40051454305648804\n",
      "Loss at epoch 673: 0.4004659950733185\n",
      "Loss at epoch 674: 0.40041762590408325\n",
      "Loss at epoch 675: 0.4003693163394928\n",
      "Loss at epoch 676: 0.40032118558883667\n",
      "Loss at epoch 677: 0.4002731144428253\n",
      "Loss at epoch 678: 0.4002252221107483\n",
      "Loss at epoch 679: 0.4001775085926056\n",
      "Loss at epoch 680: 0.40012985467910767\n",
      "Loss at epoch 681: 0.40008237957954407\n",
      "Loss at epoch 682: 0.40003502368927\n",
      "Loss at epoch 683: 0.3999877870082855\n",
      "Loss at epoch 684: 0.39994069933891296\n",
      "Loss at epoch 685: 0.39989370107650757\n",
      "Loss at epoch 686: 0.3998468816280365\n",
      "Loss at epoch 687: 0.3998001515865326\n",
      "Loss at epoch 688: 0.3997535705566406\n",
      "Loss at epoch 689: 0.3997071087360382\n",
      "Loss at epoch 690: 0.3996608257293701\n",
      "Loss at epoch 691: 0.3996146023273468\n",
      "Loss at epoch 692: 0.3995685577392578\n",
      "Loss at epoch 693: 0.399522602558136\n",
      "Loss at epoch 694: 0.3994767665863037\n",
      "Loss at epoch 695: 0.3994310200214386\n",
      "Loss at epoch 696: 0.3993855118751526\n",
      "Loss at epoch 697: 0.39934003353118896\n",
      "Loss at epoch 698: 0.3992947041988373\n",
      "Loss at epoch 699: 0.39924952387809753\n",
      "Loss at epoch 700: 0.3992044925689697\n",
      "Loss at epoch 701: 0.3991595506668091\n",
      "Loss at epoch 702: 0.3991146683692932\n",
      "Loss at epoch 703: 0.3990699350833893\n",
      "Loss at epoch 704: 0.3990253806114197\n",
      "Loss at epoch 705: 0.39898091554641724\n",
      "Loss at epoch 706: 0.39893656969070435\n",
      "Loss at epoch 707: 0.3988923728466034\n",
      "Loss at epoch 708: 0.3988482654094696\n",
      "Loss at epoch 709: 0.39880427718162537\n",
      "Loss at epoch 710: 0.39876046776771545\n",
      "Loss at epoch 711: 0.39871668815612793\n",
      "Loss at epoch 712: 0.39867308735847473\n",
      "Loss at epoch 713: 0.3986295461654663\n",
      "Loss at epoch 714: 0.3985861837863922\n",
      "Loss at epoch 715: 0.3985429108142853\n",
      "Loss at epoch 716: 0.3984997570514679\n",
      "Loss at epoch 717: 0.39845675230026245\n",
      "Loss at epoch 718: 0.3984138071537018\n",
      "Loss at epoch 719: 0.39837098121643066\n",
      "Loss at epoch 720: 0.3983282744884491\n",
      "Loss at epoch 721: 0.39828577637672424\n",
      "Loss at epoch 722: 0.3982433080673218\n",
      "Loss at epoch 723: 0.3982009291648865\n",
      "Loss at epoch 724: 0.3981586694717407\n",
      "Loss at epoch 725: 0.3981165885925293\n",
      "Loss at epoch 726: 0.39807456731796265\n",
      "Loss at epoch 727: 0.39803269505500793\n",
      "Loss at epoch 728: 0.397990882396698\n",
      "Loss at epoch 729: 0.39794921875\n",
      "Loss at epoch 730: 0.39790770411491394\n",
      "Loss at epoch 731: 0.39786621928215027\n",
      "Loss at epoch 732: 0.39782488346099854\n",
      "Loss at epoch 733: 0.39778366684913635\n",
      "Loss at epoch 734: 0.3977425694465637\n",
      "Loss at epoch 735: 0.39770159125328064\n",
      "Loss at epoch 736: 0.39766067266464233\n",
      "Loss at epoch 737: 0.39761990308761597\n",
      "Loss at epoch 738: 0.39757922291755676\n",
      "Loss at epoch 739: 0.3975386619567871\n",
      "Loss at epoch 740: 0.397498220205307\n",
      "Loss at epoch 741: 0.3974578082561493\n",
      "Loss at epoch 742: 0.3974176049232483\n",
      "Loss at epoch 743: 0.39737746119499207\n",
      "Loss at epoch 744: 0.3973374366760254\n",
      "Loss at epoch 745: 0.39729753136634827\n",
      "Loss at epoch 746: 0.3972576856613159\n",
      "Loss at epoch 747: 0.3972179889678955\n",
      "Loss at epoch 748: 0.39717841148376465\n",
      "Loss at epoch 749: 0.3971388339996338\n",
      "Loss at epoch 750: 0.39709949493408203\n",
      "Loss at epoch 751: 0.3970601558685303\n",
      "Loss at epoch 752: 0.39702099561691284\n",
      "Loss at epoch 753: 0.39698195457458496\n",
      "Loss at epoch 754: 0.3969429135322571\n",
      "Loss at epoch 755: 0.39690402150154114\n",
      "Loss at epoch 756: 0.39686527848243713\n",
      "Loss at epoch 757: 0.3968265950679779\n",
      "Loss at epoch 758: 0.39678800106048584\n",
      "Loss at epoch 759: 0.3967495858669281\n",
      "Loss at epoch 760: 0.396711140871048\n",
      "Loss at epoch 761: 0.39667293429374695\n",
      "Loss at epoch 762: 0.3966347575187683\n",
      "Loss at epoch 763: 0.3965967297554016\n",
      "Loss at epoch 764: 0.3965586721897125\n",
      "Loss at epoch 765: 0.3965208828449249\n",
      "Loss at epoch 766: 0.39648306369781494\n",
      "Loss at epoch 767: 0.3964454233646393\n",
      "Loss at epoch 768: 0.396407812833786\n",
      "Loss at epoch 769: 0.39637038111686707\n",
      "Loss at epoch 770: 0.3963330090045929\n",
      "Loss at epoch 771: 0.3962957262992859\n",
      "Loss at epoch 772: 0.39625853300094604\n",
      "Loss at epoch 773: 0.39622145891189575\n",
      "Loss at epoch 774: 0.3961844742298126\n",
      "Loss at epoch 775: 0.39614757895469666\n",
      "Loss at epoch 776: 0.39611080288887024\n",
      "Loss at epoch 777: 0.3960740864276886\n",
      "Loss at epoch 778: 0.3960374891757965\n",
      "Loss at epoch 779: 0.39600104093551636\n",
      "Loss at epoch 780: 0.3959645926952362\n",
      "Loss at epoch 781: 0.3959282636642456\n",
      "Loss at epoch 782: 0.39589208364486694\n",
      "Loss at epoch 783: 0.39585593342781067\n",
      "Loss at epoch 784: 0.39581993222236633\n",
      "Loss at epoch 785: 0.3957839608192444\n",
      "Loss at epoch 786: 0.3957481384277344\n",
      "Loss at epoch 787: 0.39571237564086914\n",
      "Loss at epoch 788: 0.39567670226097107\n",
      "Loss at epoch 789: 0.3956412076950073\n",
      "Loss at epoch 790: 0.3956056535243988\n",
      "Loss at epoch 791: 0.395570307970047\n",
      "Loss at epoch 792: 0.39553502202033997\n",
      "Loss at epoch 793: 0.3954998254776001\n",
      "Loss at epoch 794: 0.395464688539505\n",
      "Loss at epoch 795: 0.3954296410083771\n",
      "Loss at epoch 796: 0.3953947424888611\n",
      "Loss at epoch 797: 0.39535990357398987\n",
      "Loss at epoch 798: 0.3953251838684082\n",
      "Loss at epoch 799: 0.39529046416282654\n",
      "Loss at epoch 800: 0.3952558934688568\n",
      "Loss at epoch 801: 0.39522141218185425\n",
      "Loss at epoch 802: 0.39518702030181885\n",
      "Loss at epoch 803: 0.395152747631073\n",
      "Loss at epoch 804: 0.39511847496032715\n",
      "Loss at epoch 805: 0.395084410905838\n",
      "Loss at epoch 806: 0.3950502574443817\n",
      "Loss at epoch 807: 0.3950163722038269\n",
      "Loss at epoch 808: 0.3949825167655945\n",
      "Loss at epoch 809: 0.39494872093200684\n",
      "Loss at epoch 810: 0.39491501450538635\n",
      "Loss at epoch 811: 0.3948814272880554\n",
      "Loss at epoch 812: 0.39484792947769165\n",
      "Loss at epoch 813: 0.39481446146965027\n",
      "Loss at epoch 814: 0.39478111267089844\n",
      "Loss at epoch 815: 0.3947478234767914\n",
      "Loss at epoch 816: 0.3947146534919739\n",
      "Loss at epoch 817: 0.39468154311180115\n",
      "Loss at epoch 818: 0.39464855194091797\n",
      "Loss at epoch 819: 0.3946155905723572\n",
      "Loss at epoch 820: 0.39458274841308594\n",
      "Loss at epoch 821: 0.3945499658584595\n",
      "Loss at epoch 822: 0.39451733231544495\n",
      "Loss at epoch 823: 0.39448466897010803\n",
      "Loss at epoch 824: 0.39445218443870544\n",
      "Loss at epoch 825: 0.39441975951194763\n",
      "Loss at epoch 826: 0.394387423992157\n",
      "Loss at epoch 827: 0.3943551480770111\n",
      "Loss at epoch 828: 0.3943229913711548\n",
      "Loss at epoch 829: 0.39429086446762085\n",
      "Loss at epoch 830: 0.39425885677337646\n",
      "Loss at epoch 831: 0.39422690868377686\n",
      "Loss at epoch 832: 0.39419499039649963\n",
      "Loss at epoch 833: 0.3941632807254791\n",
      "Loss at epoch 834: 0.39413154125213623\n",
      "Loss at epoch 835: 0.3940999209880829\n",
      "Loss at epoch 836: 0.3940683603286743\n",
      "Loss at epoch 837: 0.3940369188785553\n",
      "Loss at epoch 838: 0.39400553703308105\n",
      "Loss at epoch 839: 0.3939741849899292\n",
      "Loss at epoch 840: 0.3939429819583893\n",
      "Loss at epoch 841: 0.39391183853149414\n",
      "Loss at epoch 842: 0.39388078451156616\n",
      "Loss at epoch 843: 0.39384976029396057\n",
      "Loss at epoch 844: 0.39381885528564453\n",
      "Loss at epoch 845: 0.39378803968429565\n",
      "Loss at epoch 846: 0.39375728368759155\n",
      "Loss at epoch 847: 0.3937266170978546\n",
      "Loss at epoch 848: 0.39369601011276245\n",
      "Loss at epoch 849: 0.3936654329299927\n",
      "Loss at epoch 850: 0.3936350345611572\n",
      "Loss at epoch 851: 0.3936046361923218\n",
      "Loss at epoch 852: 0.3935743272304535\n",
      "Loss at epoch 853: 0.39354407787323\n",
      "Loss at epoch 854: 0.393513947725296\n",
      "Loss at epoch 855: 0.3934839367866516\n",
      "Loss at epoch 856: 0.3934538960456848\n",
      "Loss at epoch 857: 0.3934239447116852\n",
      "Loss at epoch 858: 0.3933941125869751\n",
      "Loss at epoch 859: 0.3933643102645874\n",
      "Loss at epoch 860: 0.39333465695381165\n",
      "Loss at epoch 861: 0.3933050334453583\n",
      "Loss at epoch 862: 0.3932754397392273\n",
      "Loss at epoch 863: 0.39324599504470825\n",
      "Loss at epoch 864: 0.3932165801525116\n",
      "Loss at epoch 865: 0.39318719506263733\n",
      "Loss at epoch 866: 0.3931578993797302\n",
      "Loss at epoch 867: 0.39312878251075745\n",
      "Loss at epoch 868: 0.3930996358394623\n",
      "Loss at epoch 869: 0.39307060837745667\n",
      "Loss at epoch 870: 0.3930416703224182\n",
      "Loss at epoch 871: 0.39301276206970215\n",
      "Loss at epoch 872: 0.39298391342163086\n",
      "Loss at epoch 873: 0.39295512437820435\n",
      "Loss at epoch 874: 0.392926424741745\n",
      "Loss at epoch 875: 0.3928978443145752\n",
      "Loss at epoch 876: 0.3928692936897278\n",
      "Loss at epoch 877: 0.39284077286720276\n",
      "Loss at epoch 878: 0.39281243085861206\n",
      "Loss at epoch 879: 0.39278408885002136\n",
      "Loss at epoch 880: 0.39275580644607544\n",
      "Loss at epoch 881: 0.39272764325141907\n",
      "Loss at epoch 882: 0.3926995098590851\n",
      "Loss at epoch 883: 0.3926714062690735\n",
      "Loss at epoch 884: 0.39264342188835144\n",
      "Loss at epoch 885: 0.39261552691459656\n",
      "Loss at epoch 886: 0.39258766174316406\n",
      "Loss at epoch 887: 0.39255988597869873\n",
      "Loss at epoch 888: 0.3925321698188782\n",
      "Loss at epoch 889: 0.3925045132637024\n",
      "Loss at epoch 890: 0.3924769163131714\n",
      "Loss at epoch 891: 0.39244943857192993\n",
      "Loss at epoch 892: 0.3924219608306885\n",
      "Loss at epoch 893: 0.39239463210105896\n",
      "Loss at epoch 894: 0.39236727356910706\n",
      "Loss at epoch 895: 0.3923400044441223\n",
      "Loss at epoch 896: 0.3923128843307495\n",
      "Loss at epoch 897: 0.3922857344150543\n",
      "Loss at epoch 898: 0.3922587037086487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 899: 0.39223170280456543\n",
      "Loss at epoch 900: 0.39220476150512695\n",
      "Loss at epoch 901: 0.392177939414978\n",
      "Loss at epoch 902: 0.3921511471271515\n",
      "Loss at epoch 903: 0.3921244144439697\n",
      "Loss at epoch 904: 0.39209774136543274\n",
      "Loss at epoch 905: 0.3920712172985077\n",
      "Loss at epoch 906: 0.39204463362693787\n",
      "Loss at epoch 907: 0.3920181691646576\n",
      "Loss at epoch 908: 0.3919917941093445\n",
      "Loss at epoch 909: 0.39196547865867615\n",
      "Loss at epoch 910: 0.3919391930103302\n",
      "Loss at epoch 911: 0.39191296696662903\n",
      "Loss at epoch 912: 0.3918868601322174\n",
      "Loss at epoch 913: 0.3918607532978058\n",
      "Loss at epoch 914: 0.3918347656726837\n",
      "Loss at epoch 915: 0.39180874824523926\n",
      "Loss at epoch 916: 0.39178287982940674\n",
      "Loss at epoch 917: 0.391757071018219\n",
      "Loss at epoch 918: 0.391731321811676\n",
      "Loss at epoch 919: 0.39170557260513306\n",
      "Loss at epoch 920: 0.39167994260787964\n",
      "Loss at epoch 921: 0.3916544020175934\n",
      "Loss at epoch 922: 0.39162883162498474\n",
      "Loss at epoch 923: 0.39160338044166565\n",
      "Loss at epoch 924: 0.3915780186653137\n",
      "Loss at epoch 925: 0.3915526866912842\n",
      "Loss at epoch 926: 0.391527384519577\n",
      "Loss at epoch 927: 0.39150217175483704\n",
      "Loss at epoch 928: 0.3914770185947418\n",
      "Loss at epoch 929: 0.391451895236969\n",
      "Loss at epoch 930: 0.39142686128616333\n",
      "Loss at epoch 931: 0.3914019465446472\n",
      "Loss at epoch 932: 0.39137697219848633\n",
      "Loss at epoch 933: 0.391352117061615\n",
      "Loss at epoch 934: 0.3913273513317108\n",
      "Loss at epoch 935: 0.39130261540412903\n",
      "Loss at epoch 936: 0.391277939081192\n",
      "Loss at epoch 937: 0.3912533223628998\n",
      "Loss at epoch 938: 0.39122873544692993\n",
      "Loss at epoch 939: 0.39120423793792725\n",
      "Loss at epoch 940: 0.39117977023124695\n",
      "Loss at epoch 941: 0.3911554217338562\n",
      "Loss at epoch 942: 0.39113110303878784\n",
      "Loss at epoch 943: 0.3911067843437195\n",
      "Loss at epoch 944: 0.3910825848579407\n",
      "Loss at epoch 945: 0.39105844497680664\n",
      "Loss at epoch 946: 0.3910343647003174\n",
      "Loss at epoch 947: 0.3910103440284729\n",
      "Loss at epoch 948: 0.3909863531589508\n",
      "Loss at epoch 949: 0.3909624218940735\n",
      "Loss at epoch 950: 0.39093858003616333\n",
      "Loss at epoch 951: 0.3909147381782532\n",
      "Loss at epoch 952: 0.3908909857273102\n",
      "Loss at epoch 953: 0.39086729288101196\n",
      "Loss at epoch 954: 0.39084362983703613\n",
      "Loss at epoch 955: 0.39082005620002747\n",
      "Loss at epoch 956: 0.3907965123653412\n",
      "Loss at epoch 957: 0.3907730281352997\n",
      "Loss at epoch 958: 0.39074963331222534\n",
      "Loss at epoch 959: 0.390726238489151\n",
      "Loss at epoch 960: 0.3907029330730438\n",
      "Loss at epoch 961: 0.3906797170639038\n",
      "Loss at epoch 962: 0.3906565010547638\n",
      "Loss at epoch 963: 0.39063334465026855\n",
      "Loss at epoch 964: 0.3906102776527405\n",
      "Loss at epoch 965: 0.3905872404575348\n",
      "Loss at epoch 966: 0.3905642628669739\n",
      "Loss at epoch 967: 0.39054131507873535\n",
      "Loss at epoch 968: 0.3905184268951416\n",
      "Loss at epoch 969: 0.3904956579208374\n",
      "Loss at epoch 970: 0.3904728889465332\n",
      "Loss at epoch 971: 0.3904501497745514\n",
      "Loss at epoch 972: 0.39042747020721436\n",
      "Loss at epoch 973: 0.3904048800468445\n",
      "Loss at epoch 974: 0.3903822898864746\n",
      "Loss at epoch 975: 0.3903597891330719\n",
      "Loss at epoch 976: 0.39033737778663635\n",
      "Loss at epoch 977: 0.3903149366378784\n",
      "Loss at epoch 978: 0.3902926445007324\n",
      "Loss at epoch 979: 0.3902703523635864\n",
      "Loss at epoch 980: 0.3902480900287628\n",
      "Loss at epoch 981: 0.39022591710090637\n",
      "Loss at epoch 982: 0.3902037739753723\n",
      "Loss at epoch 983: 0.39018169045448303\n",
      "Loss at epoch 984: 0.39015963673591614\n",
      "Loss at epoch 985: 0.3901376724243164\n",
      "Loss at epoch 986: 0.39011573791503906\n",
      "Loss at epoch 987: 0.3900938332080841\n",
      "Loss at epoch 988: 0.3900720477104187\n",
      "Loss at epoch 989: 0.3900502324104309\n",
      "Loss at epoch 990: 0.3900285065174103\n",
      "Loss at epoch 991: 0.3900068700313568\n",
      "Loss at epoch 992: 0.38998523354530334\n",
      "Loss at epoch 993: 0.38996362686157227\n",
      "Loss at epoch 994: 0.3899421691894531\n",
      "Loss at epoch 995: 0.3899206519126892\n",
      "Loss at epoch 996: 0.38989919424057007\n",
      "Loss at epoch 997: 0.3898778557777405\n",
      "Loss at epoch 998: 0.3898565471172333\n",
      "Loss at epoch 999: 0.3898352384567261\n",
      "Loss at epoch 1000: 0.38981398940086365\n"
     ]
    }
   ],
   "source": [
    "# define the number of epochs for both plain and encrypted training\n",
    "EPOCHS = 1000\n",
    "torch.random.manual_seed(0)\n",
    "random.seed(0)\n",
    "def train(model, optim, criterion, x, y, epochs=EPOCHS):\n",
    "    for e in range(1, epochs + 1):\n",
    "        optim.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f\"Loss at epoch {e}: {loss.data}\")\n",
    "    return model\n",
    "\n",
    "model = train(model, optim, criterion, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64585342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on plain test_set: 0.8473643064498901\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, x, y):\n",
    "    out = model(x)\n",
    "    correct = torch.abs(y - out) < 0.5\n",
    "    return correct.float().mean()\n",
    "\n",
    "plain_accuracy = accuracy(model, x_test, y_test)\n",
    "print(f\"Accuracy on plain test_set: {plain_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4db87e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR:\n",
    "    \n",
    "    def __init__(self, torch_lr):\n",
    "        # TenSEAL processes lists and not torch tensors,\n",
    "        # so we take out the parameters from the PyTorch model\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()\n",
    "        \n",
    "    def forward(self, enc_x):\n",
    "        # We don't need to perform sigmoid as this model\n",
    "        # will only be used for evaluation, and the label\n",
    "        # can be deduced without applying sigmoid\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        return enc_out\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "        \n",
    "    ################################################\n",
    "    ## You can use the functions below to perform ##\n",
    "    ## the evaluation with an encrypted model     ##\n",
    "    ################################################\n",
    "    \n",
    "    def encrypt(self, context):\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "        \n",
    "    def decrypt(self, context):\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "        \n",
    "\n",
    "eelr = EncryptedLR(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f743546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "poly_mod_degree = 4096\n",
    "coeff_mod_bit_sizes = [40, 20, 40]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 20\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7adf1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption of the test-set took 2 seconds\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "enc_x_test = [ts.ckks_vector(ctx_eval, x.tolist()) for x in x_test]\n",
    "t_end = time()\n",
    "print(f\"Encryption of the test-set took {int(t_end - t_start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6bdd031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated test_set of 1271 entries in 5 seconds\n",
      "Accuracy: 1081/1271 = 0.8505114083398898\n",
      "Difference between plain and encrypted accuracies: -0.003147125244140625\n",
      "Oh! We got a better accuracy on the encrypted test-set! The noise was on our side...\n"
     ]
    }
   ],
   "source": [
    "def encrypted_evaluation(model, enc_x_test, y_test):\n",
    "    t_start = time()\n",
    "    \n",
    "    correct = 0\n",
    "    for enc_x, y in zip(enc_x_test, y_test):\n",
    "        # encrypted evaluation\n",
    "        enc_out = model(enc_x)\n",
    "        # plain comparison\n",
    "        out = enc_out.decrypt()\n",
    "        out = torch.tensor(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        if torch.abs(out - y) < 0.5:\n",
    "            correct += 1\n",
    "    \n",
    "    t_end = time()\n",
    "    print(f\"Evaluated test_set of {len(x_test)} entries in {int(t_end - t_start)} seconds\")\n",
    "    print(f\"Accuracy: {correct}/{len(x_test)} = {correct / len(x_test)}\")\n",
    "    return correct / len(x_test)\n",
    "    \n",
    "\n",
    "encrypted_accuracy = encrypted_evaluation(eelr, enc_x_test, y_test)\n",
    "diff_accuracy = plain_accuracy - encrypted_accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
    "if diff_accuracy < 0:\n",
    "    print(\"Oh! We got a better accuracy on the encrypted test-set! The noise was on our side...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
